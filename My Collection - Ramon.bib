@misc{WebDocs,
author = {{Web Docs}, MDN},
title = {{Introducci{\'{o}}n a Express/Node}},
url = {https://developer.mozilla.org/es/docs/Learn/Server-side/Express{\_}Nodejs/Introduction},
urldate = {28-01-2019}
}
@misc{Foundation,
author = {jQuery Foundation, The},
title = {{JQuery}},
url = {http://jquery.com},
urldate = {28-01-2019}
}
@misc{,
title = {{No Title}},
url = {http://getbootstrap.com/},
urldate = {2019-01-29}
}
@misc{UniversitatdeBarcelona,
author = {{Universitat de Barcelona}},
title = {{Bootstrap ¿que es y para qu{\'{e}} sirve?}},
url = {https://www.obs-edu.com/int/blog-investigacion/sistemas/bootstrap-que-es-y-para-que-sirve},
urldate = {29-01-2019}
}
@book{,
file = {:Users/Raymond/Universidad/TESIS/AL/Fundamentos{\_}de{\_}Bases{\_}de{\_}Datos{\_}5a{\_}Ed.pdf:pdf},
isbn = {0072958863},
title = {{No Title}}
}
@misc{Chacon,
author = {Chacon, Scott},
title = {{Git --local-branching-on-the-cheap}},
url = {https://git-scm.com/book/es/v1/Empezando-Fundamentos-de-Git},
urldate = {29-01-2019}
}
@article{Joskowicz2008,
author = {Joskowicz, Ing Jos{\'{e}} and Mingus, Charles},
file = {:Users/Raymond/Universidad/TESIS/AL/Reglas y Pr{\'{a}}cticas en eXtreme Programming.pdf:pdf},
pages = {1--22},
title = {{Reglas y Pr{\'{a}}cticas en eXtreme Programming}},
year = {2008}
}
@phdthesis{BellidoSanchez,
author = {{Bellido Sanchez}, Sergio},
file = {:Users/Raymond/Universidad/TESIS/AL/PFC Sergio Bellido Sanchez 252F Tema 5 Mongo DB.pdf:pdf},
keywords = {a continuaci{\'{o}}n girar{\'{a}} en,a la arquitectura de,amazon con su dinamodb,basados en esta tecnolog{\'{i}}a,bigtable,cap{\'{i}}tulo que se inicia,destacar,el contenido de este,entre los que caben,google con su,mongodb,o linkedin con redis,promesa de mongodb de,se profundizar{\'{a}} en la,torno},
pages = {42--57},
school = {Universidad de Sevilla},
title = {{e-Assessment mediante Bases de Datos NoSQL}},
url = {http://bibing.us.es/proyectos/abreproy/12037/fichero/PFC{\_}Sergio{\_}Bellido{\_}Sanchez{\%}252FTema5{\_}mongodb.pdf}
}
@book{SILBERSCHATZ2006,
author = {SILBERSCHATZ, ABRAHAM and de Yale, Universidad and KORTH, HENRY F. and de Lehigh, Universidad and SUDARSHAN, S. and Instituto tecnológico indio, Bombay},
file = {:Users/Raymond/Universidad/TESIS/AL/Fundamentos{\_}de{\_}Bases{\_}de{\_}Datos{\_}5a{\_}Ed.pdf:pdf},
title = {{Fundamentos de Bases de Datos 5a Edicion.pdf}},
year = {2006}
}
@misc{W3C,
author = {W3C},
title = {{http://www.w3.org/XML}},
url = {http://www.w3.org/XML},
urldate = {23-03-2019}
}
@misc{Oracle,
author = {Oracle},
title = {{Sun {\&} Oracle}},
url = {http://www.oracle.com/us/sun/index.html},
urldate = {2019-03-08}
}
@misc{Foundationa,
author = {Foundation, Node.js},
title = {{Node.js}},
url = {https://nodejs.org/es/about/},
urldate = {2019-01-28}
}
@misc{Oraclea,
author = {Oracle},
title = {{No Title}},
url = {https://www.oracle.com/technetwork/java/javase/12u-relnotes-5211424.html},
urldate = {8/3/2019}
}
@book{Chaffer2013,
author = {Chaffer, Jonathan and Swedberg, Karl},
title = {{Learning jQuery - Fourth Edition}},
year = {2013}
}
@misc{Microsoft,
author = {Microsoft},
title = {https://code.visualstudio.com/docs/editor/whyvscode},
urldate = {2019-03-10}
}
@book{jacobson2000proceso,
author = {Jacobson, I and Booch, G and Rumbaugh, J},
isbn = {9788478290369},
publisher = {Pearson Educaci{\'{o}}n},
series = {Fuera de colecci{\'{o}}n Out of series},
title = {{El proceso unificado de desarrollo de software}},
url = {https://books.google.com.cu/books?id=zHKbQgAACAAJ},
year = {2000}
}
@book{Rosenberg,
author = {Rosenberg, Doug and Stephens, Matt},
file = {:Users/Raymond/Universidad/TESIS/AL/epdf.tips{\_}use-case-driven-object-modeling-with-uml-theory-an.pdf:pdf},
isbn = {9781590597743},
title = {{Use Case Driven Object Modeling with UML Theory and Practice}}
}
@article{International2009,
author = {International, Ecma},
file = {:Users/Raymond/Universidad/TESIS/AL/ECMAScript{\textregistered} 2018 Language Specification.pdf:pdf},
title = {{ECMA-262}},
year = {2009}
}
@article{DavidFlanagan2004,
author = {{David Flanagan}},
file = {:Users/Raymond/Universidad/TESIS/AL/JavaScript - The Definitive Guide.pdf:pdf},
title = {{JavaScript : The Definitive Guide , 4th Edition By David Flanagan Lilmeanman}},
year = {2004}
}
@misc{WorldWideWebConsortium,
author = {{World Wide Web Consortium}},
title = {https://www.w3.org/html/},
url = {https://www.w3.org/html/}
}
@misc{Mozilla,
author = {Mozilla},
title = {{CSS: Cascading Style Sheets | MDN}},
url = {https://developer.mozilla.org/en-US/docs/Web/CSS},
urldate = {2019-03-07}
}
@article{Zhang2003,
abstract = {In the task of adaptive information filtering,  a system receives a stream of documents but  delivers only those that match a person's information  need. As the system filters it also  refines its knowledge about the user's information  needs based on relevance feedback  from the user. Delivering a document thus  has two effects: i) it satisfies the user's information  need immediately, and ii) it helps  the system better satisfy the user in the future  by improving its model of the user's information  need. The traditional approach to  adaptive information filtering fails to recognize  and model this second e{\#}ect.},
author = {Zhang, Yi and Xu, Wei and Callan, Jamie},
doi = {10.1.1.8.375},
file = {:Users/Raymond/Universidad/TESIS/AL/Exploration and Exploitation in Adaptive Filtering Based on Bayesian Active Learning.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
pages = {896--903},
title = {{Exploration and Exploitation in Adaptive Filtering Based on Bayesian Active Learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.375},
year = {2003}
}
@article{Culotta2006,
abstract = {To successfully embed statistical machine learning models in real world applications, two post-deployment capabilities must be provided: (1) the ability to solicit user corrections and (2) the ability to update the model from these corrections. We refer to the former capability as corrective feedback and the latter as persistent learning. While these capabilities have a natural implementation for simple classification tasks such as spam filtering, we argue that a more careful design is required for structured classification tasks. One example of a structured classification task is information extraction, in which raw text is analyzed to automatically populate a database. In this work, we augment a probabilistic information extraction system with corrective feedback and persistent learning components to assist the user in building, correcting, and updating the extraction model. We describe methods of guiding the user to incorrect predictions, suggesting the most informative fields to correct, and incorporating corrections into the inference algorithm. We also present an active learning framework that minimizes not only how many examples a user must label, but also how difficult each example is to label. We empirically validate each of the technical components in simulation and quantify the user effort saved. We conclude that more efficient corrective feedback mechanisms lead to more effective persistent learning. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Culotta, Aron and Kristjansson, Trausti and McCallum, Andrew and Viola, Paul},
doi = {10.1016/j.artint.2006.08.001},
file = {:Users/Raymond/Universidad/TESIS/AL/Corrective Feedback and Persistent Learning for Information Extraction.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Active learning,Graphical models,Information extraction},
number = {14-15},
pages = {1101--1122},
title = {{Corrective feedback and persistent learning for information extraction}},
volume = {170},
year = {2006}
}
@article{Liere1997,
abstract = {In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant. We report here on experiments using a committee of Winnowbased learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by 1-2 orders of magnitude.},
author = {Liere, Ray and Tadepalli, Prasad},
file = {:Users/Raymond/Universidad/TESIS/AL/ Active Learning with Committees for Text Categorization.pdf:pdf},
journal = {In proceedings of the Fourteenth National Conference on Artificial Intelligence},
pages = {591--596},
title = {{Active learning with committees for text categorization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.2349},
year = {1997}
}
@misc{MacKay,
author = {MacKay, David J. C.},
file = {:Users/Raymond/Universidad/TESIS/AL/Information 􏰀based objective functions for active data selection.pdf:pdf},
title = {{Information based objective functions for active data selection.pdf}}
}
@article{Tsoumakas2011,
author = {Tsoumakas, Grigorios and Vilcek, Jozef},
file = {:Users/Raymond/Universidad/TESIS/AL/MULAN- A Java Library for Multi-Label Learning.pdf:pdf},
keywords = {1,a multi-label data set,a subset of a,archical classification,classification,consists of training examples,dimensionality reduction,evaluation,finite set,hier-,multi-label data,multi-label learning,ranking,that are associated with,thresholding},
pages = {2411--2414},
title = {{MULAN : A Java Library for Multi-Label Learning}},
volume = {12},
year = {2011}
}
@article{Freund1997,
abstract = {We analyze the "query by committee" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.},
author = {Freund, Yoav and SEUNG, H. SEBASTIAN and Shamir, Eli and Tishby, Naftali},
doi = {10.1.1.20.8521},
file = {:Users/Raymond/Universidad/TESIS/AL/Selective Sampling Using the Query by Committee Algorithm.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {bayesian learning,experimental design,query learning,selective sampling},
pages = {133--168},
pmid = {4060604},
title = {{Selective Sampling Using the Query by Committee}},
volume = {168},
year = {1997}
}
@article{Ventura:2007:JJF:1315745.1315749,
address = {Berlin, Heidelberg},
author = {Ventura, Sebastian and Romero, Cristobal and Zafra, Amelia and Delgado, Jose; A and Herves, Cesar},
doi = {10.1007/s00500-007-0172-0},
issn = {1432-7643},
journal = {Soft Comput.},
keywords = {Evolutionary computation software tools,Framework,Java,Object oriented design},
month = {oct},
number = {4},
pages = {381--392},
publisher = {Springer-Verlag},
title = {{JCLEC: A Java Framework for Evolutionary Computation}},
url = {https://doi.org/10.1007/s00500-007-0172-0},
volume = {12},
year = {2007}
}
@article{Hakkani-Tur2011,
abstract = {State-of-the-art speech recognition systems are trained using transcribed utterances, preparation of which is labor intensive and time-consuming. In this paper, we describe a new method for reducing the transcription effort for training in automatic speech recognition (ASR). Active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples, and then selecting the most informative ones with respect to a given cost function for a human to label. We automatically estimate a confidence score for each word of the utterance, exploiting the lattice output of a speech recognizer, which was trained on a small set of transcribed data. We compute utterance confidence scores based on these word confidence scores, then selectively sample the utterances to be transcribed using the utterance confidence scores. In our experiments, we show that we reduce the amount of labeled data needed for a given word accuracy by 27{\%}.},
author = {Hakkani-Tur, Dilek and Riccardi, Giuseppe and Gorin, Allen},
doi = {10.1109/icassp.2002.5745510},
file = {:Users/Raymond/Universidad/TESIS/AL/Hakkani-TurRiccardiGorin-2002-ActiveLearningforAutomaticSpeechRecognition.pdf:pdf},
isbn = {0-7803-0946-4},
number = {December 2016},
pages = {IV--3904--IV--3907},
title = {{Active learning for automatic speech recognition}},
year = {2011}
}
@article{Thompson1999,
author = {Thompson, Cynthia A and Hall, Ventura and Mooney, Raymond J},
file = {:Users/Raymond/Universidad/TESIS/AL/Active Learning for Natural Language Parsing and Information Extraction.pdf:pdf},
journal = {Machine Learning},
number = {June},
pages = {406--414},
title = {{Active Learning for Natural Language Parsing and Information Extraction LEARNING SYSTEMS}},
year = {1999}
}
@inproceedings{Seung2004,
abstract = {We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.},
author = {Seung, H. S. and Opper, M. and Sompolinsky, H.},
doi = {10.1145/130385.130417},
file = {:Users/Raymond/Universidad/TESIS/AL/Query by Committee.pdf:pdf},
isbn = {089791497X},
pages = {287--294},
title = {{Query by committee}},
year = {2004}
}
@article{Huang2018,
abstract = {Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.},
author = {Huang, Shujun and Nianguang, C. A.I. and {Penzuti Pacheco}, Pedro and Narandes, Shavira and Wang, Yang and Wayne, X. U.},
doi = {10.21873/cgp.20063},
file = {:Users/Raymond/Universidad/TESIS/AL/Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.pdf:pdf},
issn = {17906245},
journal = {Cancer Genomics and Proteomics},
keywords = {Biomarker discovery,Cancer classification,Classifier,Driver gene,Drug discovery,Gene expression,Gene selection,Gene-gene interaction,Genomics,Kernel function,Machine learning (ML),Review,Support vector machine (SVM)},
number = {1},
pages = {41--51},
pmid = {29275361},
title = {{Applications of support vector machine (SVM) learning in cancer genomics}},
volume = {15},
year = {2018}
}
@article{Tong2001,
abstract = {A novel method of relevance feedback is presented based on support vector machine learning in the content-based image retrieval system. A SVM classifier can be learned from training data of relevance images and irrelevance images marked by users. Using the classifier, the system can retrieve more images relevant to the query in the database efficiently. Experiments were carried out on a large-size database of 9918 images. It shows that the interactive learning and retrieval process can find correct images increasingly. It also shows the generalization ability of SVM under the condition of limited training samples},
author = {Tong, Simon and Chang, Edward},
doi = {10.1145/500141.500159},
file = {:Users/Raymond/Universidad/TESIS/AL/Support Vector Machine Active Learning for Image Retrieval.pdf:pdf},
isbn = {1581133944},
journal = {Proceedings of the ninth ACM international conference on Multimedia  - MULTIMEDIA '01},
number = {C},
pages = {107},
title = {{Support vector machine active learning for image retrieval}},
url = {http://portal.acm.org/citation.cfm?doid=500141.500159},
year = {2001}
}
@article{Tong2000,
abstract = {Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using {\textless}em{\textgreater}pool-based active learning{\textless}/em{\textgreater}. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a {\textless}em{\textgreater}version space{\textless}/em{\textgreater}. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.},
author = {Tong, Simon and Koller, Daphne},
doi = {10.1162/153244302760185243},
file = {:Users/Raymond/Universidad/TESIS/AL/Support Vector Machine Active Learning with Applications to Text Classification.pdf:pdf},
isbn = {1558607072},
issn = {0003-6951},
journal = {CrossRef Listing of Deleted DOIs},
keywords = {active learning,classifica-,relevance feedback,selective sampling,support vector machines,tion},
pages = {45--66},
pmid = {8804822},
title = {{Support Vector Machine Active Learning with Applications to Text Classification Simon}},
url = {http://www.crossref.org/deleted{\_}DOI.html},
volume = {1},
year = {2000}
}
@article{Settles2010,
abstract = {Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.},
author = {Settles, Burr and Craven, Mark},
doi = {10.3115/1613715.1613855},
file = {:Users/Raymond/Universidad/TESIS/AL/An Analysis of Active Learning Strategies for Sequence Labeling Tasks.pdf:pdf},
isbn = {9788576521631},
issn = {10811206},
pages = {1070},
pmid = {2147071},
title = {{An analysis of active learning strategies for sequence labeling tasks}},
year = {2010}
}
@article{Hoi2009,
abstract = {Support vector machine (SVM) active learning is one popular and successful technique for rel-evance feedback in content-based image retrieval (CBIR). Despite the success, conventional SVM active learning has two main drawbacks. First, the performance of SVM is usually limited by the number of labeled examples. It often suffers a poor performance for the small-sized labeled examples, which is the case in relevance feedback. Second, conventional approaches do not take into account the redundancy among examples, and could select multiple examples that are similar (or even identical). In this work, we propose a novel scheme for explicitly addressing the draw-backs. It first learns a kernel function from a mixture of labeled and unlabeled data, and therefore alleviates the problem of small-sized training data. The kernel will then be used for a batch mode active learning method to identify the most informative and diverse examples via a min-max framework. Two novel algorithms are proposed to solve the related combinatorial optimization: the first approach approximates the problem into a quadratic program, and the second solves the combinatorial optimization approximately by a greedy algorithm that exploits the merits of sub-modular functions. Extensive experiments with image retrieval using both natural photo images and medical images show that the proposed algorithms are significantly more effective than the state-of-the-art approaches.},
author = {Hoi, Steven C. H. and Jin, Rong and Zhu, Jianke and Lyu, Michael R.},
doi = {10.1145/1508850.1508854},
file = {:Users/Raymond/Universidad/TESIS/AL/Batch Mode Active Learning with Applications to Text Categorization and Image Retrieval.pdf:pdf},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
number = {3},
pages = {1--29},
title = {{Semisupervised SVM batch mode active learning with applications to image retrieval}},
volume = {27},
year = {2009}
}
@article{Sassano2007,
abstract = {We explore how active learning with Support Vector Machines works well for a non-trivial task in natural language processing. We use Japanese word segmentation as a test case. In particular, we discuss how the size of a pool affects the learning curve},
author = {Sassano, Manabu},
doi = {10.3115/1073083.1073168},
file = {:Users/Raymond/Universidad/TESIS/AL/An Empirical Study of Active Learning with Support Vector Machines for Japanese Word Segmentation.pdf:pdf},
number = {July},
pages = {505},
title = {{An empirical study of active learning with support vector machines for Japanese word segmentation}},
year = {2007}
}
@misc{Lewis2007,
abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9407020},
author = {Lewis, David D.},
booktitle = {ACM SIGIR Forum},
doi = {10.1145/219587.219592},
eprint = {9407020},
file = {:Users/Raymond/Universidad/TESIS/AL/A Sequential Algorithm for Training Text Classifiers.pdf:pdf},
isbn = {978-3-540-19889-5},
issn = {01635840},
number = {2},
pages = {13--19},
pmid = {19538444},
primaryClass = {cmp-lg},
title = {{A sequential algorithm for training text classifiers}},
volume = {29},
year = {2007}
}
@article{Angluin2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Angluin, Dana},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/Raymond/Universidad/TESIS/AL/Queries and Concept Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Machine Learning},
keywords = {icle},
number = {1983},
pages = {1689--1699},
pmid = {25246403},
title = {{Queries and Concept Learning}},
volume = {53},
year = {2013}
}
@article{Kuo2007,
author = {Kuo, Jin-Shea and Li, Haizhou and Yang, Ying-Kuei},
doi = {10.3115/1220175.1220317},
file = {:Users/Raymond/Universidad/TESIS/AL/Learning Transliteration Lexicons from the Web.pdf:pdf},
number = {July},
pages = {1129--1136},
title = {{Learning transliteration lexicons from the web}},
year = {2007}
}
@article{Zhao2015,
author = {Zhao, Shanheng and Ng, Hwee Tou},
doi = {10.3115/v1/w14-1104},
file = {:Users/Raymond/Universidad/TESIS/AL/Domain Adaptation with Active Learning for Word Sense Disambiguation .pdf:pdf},
number = {June},
pages = {21--29},
title = {{Domain Adaptation with Active Learning for Coreference Resolution}},
year = {2015}
}
@article{Tur2005,
abstract = {In this paper, we describe active and semi-supervised learning methods for reducing the labeling effort for spoken language understanding. In a goal-oriented call routing system, understanding the intent of the user can be framed as a classification problem. State of the art statistical classification systems are trained using a large number of human-labeled utterances, preparation of which is labor intensive and time consuming. Active learning aims to minimize the number of labeled utterances by automatically selecting the utterances that are likely to be most informative for labeling. The method for active learning we propose, inspired by certainty-based active learning, selects the examples that the classifier is the least confident about. The examples that are classified with higher confidence scores (hence not selected by active learning) are exploited using two semi-supervised learning methods. The first method augments the training data by using the machine-labeled classes for the unlabeled utterances. The second method instead augments the classification model trained using the human-labeled utterances with the machine-labeled ones in a weighted manner. We then combine active and semi-supervised learning using selectively sampled and automatically labeled data. This enables us to exploit all collected data and alleviates the data imbalance problem caused by employing only active or semi-supervised learning. We have evaluated these active and semi-supervised learning methods with a call classification system used for AT{\&}T customer care. Our results indicate that it is possible to reduce human labeling effort significantly. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Tur, Gokhan and Hakkani-T{\"{u}}r, Dilek and Schapire, Robert E.},
doi = {10.1016/j.specom.2004.08.002},
file = {:Users/Raymond/Universidad/TESIS/AL/Combining active and semi-supervised learning for spoken language understanding.pdf:pdf},
issn = {01676393},
journal = {Speech Communication},
keywords = {Active learning,Call classification,Semi-supervised learning,Spoken language understanding},
number = {2},
pages = {171--186},
title = {{Combining active and semi-supervised learning for spoken language understanding}},
volume = {45},
year = {2005}
}
@article{Cohn1996,
abstract = {I consider the question 'How should one act when the only goal is to learn as much as possible?'. Building on the theoretical results of Fedorov (1972, Theory of Optimal Experiments, Academic Press) and MacKay (1992, Neural Computation, 4, 590-604), I apply techniques from optimal experiment design (OED) to guide the query/action selection of a neural network learner. I demonstrate that these techniques allow the learner to minimize its generalization error by exploring its domain efficiently and completely. I conclude that, while not a panacea, OED-based query/action selection has much to offer, especially in domains where its high computational costs can be tolerated.},
author = {Cohn, David A.},
doi = {10.1016/0893-6080(95)00137-9},
file = {:Users/Raymond/Universidad/TESIS/AL/ Neural Network Exploration Using Optimal Experiment Design.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {active learning,exploration,optimal experiment design,queries,uncertainty},
number = {6},
pages = {1071--1083},
title = {{Neural network exploration using optimal experiment design}},
volume = {9},
year = {1996}
}
@misc{Argamon-engelson1999,
author = {Argamon-engelson, Shlomo},
file = {:Users/Raymond/Universidad/TESIS/AL/Committee-Based Sample Selection For Probabilistic Classifiers.pdf:pdf},
pages = {335--360},
title = {{Committee-Based Sample Selection For Probabilistic Classi ers}},
volume = {11},
year = {1999}
}
@article{Hachey2010,
abstract = {We report on an active learning experi- ment for named entity recognition in the astronomy domain. Active learning has been shown to reduce the amount of la- belled data required to train a supervised learner by selectively sampling more in- formative data points for human annota- tion. We inspect double annotation data from the same domain and quantify poten- tial problems concerning annotators' per- formance. For data selectively sampled according to different selection metrics, we find lower inter-annotator agreement and higher per token annotation times. However, overall results confirm the util- ity of active learning.},
author = {Hachey, Ben and Alex, Beatrice and Becker, Markus},
doi = {10.3115/1706543.1706569},
file = {:Users/Raymond/Universidad/TESIS/AL/Investigating the Effects of Selective Sampling on the Annotation Task.pdf:pdf},
pages = {144},
title = {{Investigating the effects of selective sampling on the annotation task}},
year = {2010}
}
@article{Becker2005,
abstract = {Training a statistical named entity recognition system in a new domain requires costly man- ual annotation of large quantities of in-domain data. Active learning promises to reduce the an- notation cost by selecting only highly informa- tive data points. This paper is concerned with a real active learning experiment to bootstrap a named entity recognition system for a new do- main of radio astronomical abstracts. We evalu- ate several committee-based metrics for quanti- fying the disagreement between classiﬁers built using multiple views, and demonstrate that the choice of metric can be optimised in simulation experiments with existing annotated data from different domains. A ﬁnal evaluation shows that we gained substantial savings compared to a ran- domly sampled baseline.},
author = {Becker, M and Hachey, B and Alex, B and Grover, C},
file = {:Users/Raymond/Universidad/TESIS/AL/Optimising Selective Sampling for Bootstrapping Named Entity Recognition.pdf:pdf},
journal = {ICML2005 Workshop on},
pages = {5--11},
title = {{Optimising selective sampling for bootstrapping named entity recognition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.5091{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@article{Becker2005a,
abstract = {Active learning reduces the amount of manually an-notated sentences necessary when training state-of-the-art statistical parsers. One popular method, un-certainty sampling, selects sentences for which the parser exhibits low certainty. However, this method does not quantify confidence about the current sta-tistical model itself. In particular, we should be less confident about selection decisions based on low frequency events. We present a novel two-stage method which first targets sentences which cannot be reliably selected using uncertainty sam-pling, and then applies standard uncertainty sam-pling to the remaining sentences. An evaluation shows that this method performs better than pure uncertainty sampling, and better than an ensemble method based on bagged ensemble members only.},
author = {Becker, Markus and Osborne, Miles},
file = {:Users/Raymond/Universidad/TESIS/AL/A{\_}Two-Stage{\_}Method{\_}for{\_}Active{\_}Learning{\_}of{\_}Statisti.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {January 2005},
pages = {991--996},
title = {{A two-stage method for active learning of statistical grammars}},
year = {2005}
}
@article{Su2014,
author = {Su, Armando},
file = {:Users/Raymond/Universidad/TESIS/AL/Aprendizaje{\_}AutomC3A1tico.pdf:pdf},
number = {January 2008},
title = {{Aprendizaje Autom{\'{a}}tico}},
year = {2014}
}
@article{Ringger2007,
abstract = {In the construction of a part-of-speech an- notated corpus, we are constrained by a fixed budget. A fully annotated corpus is required, but we can afford to label only a subset. We train a Maximum Entropy Mar- kov Model tagger from a labeled subset and automatically tag the remainder. This paper addresses the question of where to focus our manual tagging efforts in order to deliver an annotation of highest quality. In this context, we find that active learning is always helpful. We focus on Query by Un- certainty (QBU) and Query by Committee (QBC) and report on experiments with sev- eral baselines and new variations of QBC and QBU, inspired by weaknesses particu- lar to their use in this application. Experi- ments on English prose and poetry test these approaches and evaluate their robust- ness. The results allow us to make recom- mendations for both types of text and raise questions that will lead to further inquiry.},
author = {Ringger, Eric and Mcclanahan, Peter and Haertel, Robbie and Busby, George and Carmen, Marc and Carroll, James and Seppi, Kevin and Lonsdale, Deryle},
file = {:Users/Raymond/Universidad/TESIS/AL/Active Learning for Part-of-Speech Tagging- Accelerating Corpus Annotation.pdf:pdf},
journal = {LAW '07 Proceedings of the Linguistic Annotation Workshop, June, 2007, Prague},
number = {June},
pages = {101--108},
title = {{Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation}},
year = {2007}
}
@article{Lujan-Mora2002,
author = {Luj{\'{a}}n-Mora, Sergio},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Luj{\'{a}}n-Mora - 2002 - Programaci{\'{o}}n de aplicaciones web historia, principios b{\'{a}}sicos y clientes web.pdf:pdf},
isbn = {978-84-8454-206-3},
keywords = {Aplicaci{\'{o}}n,Arquitectura,Cliente,DOM,Estructura,Extranet,HTML,Internet,Intranet,Javascript,Lenguaje,Lenguajes y Sistemas Inform{\'{a}}ticos,Objeto,Programaci{\'{o}}n,Script,Servidor,Sitio,Web,etiqueta},
month = {oct},
publisher = {Editorial Club Universitario},
title = {{Programaci{\'{o}}n de aplicaciones web: historia, principios b{\'{a}}sicos y clientes web}},
url = {http://rua.ua.es/dspace/handle/10045/16995},
year = {2002}
}
@phdthesis{Sun2011,
author = {Sun, Lian},
doi = {10.1360/zd-2013-43-6-1064},
file = {:Users/Raymond/Universidad/TESIS/AL/Multi-Label Dimensionality Reduction by Liang Sun.pdf:pdf},
isbn = {2013436106},
number = {August},
school = {ARIZONA STATE UNIVERSITY},
title = {{Multi-Label Dimensionality Reduction}},
year = {2011}
}
@article{Huang2012,
author = {Huang, Sheng-jun and Gao, Wei and Zhou, Zhi-hua},
file = {:Users/Raymond/Universidad/TESIS/AL/Fast Multi-Instance Multi-Label Learning Sheng-Jun Huang, Wei Gao, and Zhi-Hua Zhou, Fellow, IEEE.pdf:pdf},
number = {1},
pages = {2291--2320, 2012},
title = {{Fast Multi-Instance Multi-Label Learning}},
volume = {176},
year = {2012}
}
@inproceedings{Huang2006,
abstract = {Multi-instance multi-label learning (MIML) has achieved success in various applications, espe- cially those involving complicated learning objects. Along with the enhancing of expressive power, the cost of annotating a MIML example also increases significantly. In this paper, we propose a novel ac- tive learning approach to reduce the labeling cost of MIML. The approach actively query the most valu- able information by exploiting diversity and un- certainty in both the input and output spaces. It designs a novel query strategy for MIML objects specifically and acquires more precise information from the oracle without additional cost. Based on the queried information, the MIML model is then effectively trained by simultaneously optimiz- ing the relevance rank among instances and labels. Experiments on benchmark datasets demonstrate that the proposed approach achieves superior per- formance on various criteria. 1},
author = {Huang, Sheng-jun and Gao, Nengneng and Chen, Songcan},
file = {:Users/Raymond/Universidad/TESIS/AL/Multi-Instance Multi-Label Active Learning∗ Sheng-Jun Huang, Nengneng Gao and Songcan Chen.pdf:pdf},
keywords = {Active Learning,Multi-Instance,Multi-Label},
mendeley-tags = {Active Learning,Multi-Instance,Multi-Label},
number = {61503182},
pages = {1886--1892},
publisher = {College of Computer Science {\&} Technology, Nanjing University of Aeronautics {\&} Astronautics Collaborative Innovation Center of Novel Software Technology and Industrialization},
title = {{Multi-Instance Multi-Label Active Learning}},
year = {2006}
}
@misc{Nilsson,
author = {Nilsson, Nils J},
file = {:Users/Raymond/Universidad/TESIS/AL/AN INTRODUCTION TO MACHINE LEARNING EARLY DRAFT OF A PROPOSED TEXTBOOK Nils J􏰀 Nilsson Rob otics Lab oratory Department of Computer Science Stanford University Stanford􏰁 CA 􏰂􏰃􏰄􏰅􏰆.pdf:pdf},
keywords = {Nilsson},
title = {{AN INTRODUCTION TO MACHINE LEARNING EARLY DRAFT OF A PROPOSED TEXTBOOK Nils J? Nilsson Rob otics Lab oratory Department of Computer Science Stanford University Stanford? CA ?????.pdf}}
}
@book{,
file = {:Users/Raymond/Universidad/TESIS/AL/Machine Learning for Hackers.pdf:pdf},
isbn = {9781449303716},
title = {www.allitebooks.com}
}
@article{Settles2010a,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
eprint = {1206.5533},
file = {:Users/Raymond/Universidad/TESIS/AL/Active Learning Literature Survey Burr Settles Computer Sciences Technical Report 1648 University of Wisconsin–Madison Updated on- January 26, 2010.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {00483931},
journal = {University of Wisconsin, Madison},
keywords = {active learning,literature surv,machine learning},
number = {2},
pages = {201--221},
pmid = {15003161},
title = {{Active learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.4245{\%}7B{\&}{\%}7Drep=rep1{\%}7B{\&}{\%}7Dtype=pdf{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.4245{\&}rep=rep1{\&}type=pdf},
volume = {15},
year = {2010}
}
@article{Gonzalez2015,
author = {Gonz{\'{a}}lez, Daniel and Fuente, D E L A},
file = {:Users/Raymond/Universidad/TESIS/AL/T{\'{E}}CNICAS DE APRENDIZAJE ACTIVO EN INTELIGENCIA ARTIFICIAL.pdf:pdf},
isbn = {2813438863},
keywords = {abstract,among the organisations that,and promotion to the,constituted the black power,discurso,estados unidos,g{\'{e}}nero,movement in,nevertheless,partido pantera negra,s,s image,the black panther party,the sixties and seventies,the united states during,this,was the one,which gave more prominence,women},
title = {{Universidad Aut{\'{o}}noma de Madrid}},
year = {2015}
}
@article{Nilsson1996,
abstract = {These notes are in the process of becoming a textbook. The process is quite unfinished, and the author solicits corrections, criticisms, and suggestions from students and other readers. Although I have tried to eliminate errors, some un- doubtedly remain—caveat lector. Many typographical infelicities will no doubt me have your suggestions about topics that are too important to be left out. I hope that future versions will cover Hopfield nets, Elman nets and other re- current nets, radial basis functions, grammar and automata learning, genetic algorithms, and Bayes networks . . .. I am also collecting exercises and project suggestions which will appear in future versions. My intention is to pursue a middle ground between a theoretical textbook and one that focusses on applications. The book concentrates on the important ideas in machine learning. I do not give proofs of many of the theorems that I state, but I do give plausibility arguments and citations to formal proofs. And, I do not treat many matters that would be of practical importance in applications; the book is not a handbook of machine learning practice. Instead, my goal is to give the reader sufficient preparation to make the extensive literature on machine learning accessible. Students in my Stanford courses on machine learning have already made several useful suggestions, as have my colleague, Pat Langley, and my teaching assistants, Ron Kohavi, Karl Pfleger, Robert Allen, and Lise Getoor. persist until the final version. More material has yet to be added. Please let Some of my plans for additions and other reminders are mentioned in marginal notes.},
archivePrefix = {arXiv},
arxivId = {0904.3664v1},
author = {Nilsson, Nils J},
doi = {10.1016/j.neuroimage.2010.11.004},
eprint = {0904.3664v1},
file = {:Users/Raymond/Universidad/TESIS/AL/INTRODUCTION TO MACHINE LEARNING AN EARLY DRAFT OF A PROPOSED TEXTBOOK Nils J. Nilsson Robotics Laboratory Department of Computer Science Stanford University Stanford, CA 94305.pdf:pdf},
isbn = {9780262012430},
issn = {1095-9572},
pages = {179},
pmid = {20382237},
title = {{Introduction to Machine Learning: An Early Draft of a Proposed Textbook.}},
url = {http://ai.stanford.edu/people/nilsson/MLBOOK.pdf},
year = {1996}
}
@article{Pasillas2018,
author = {Pasillas, Andric},
file = {:Users/Raymond/Universidad/TESIS/AL/¿Qu{\'{e}} es machine learning? [Gu{\'{i}}a completa para principiantes].pdf:pdf},
pages = {1--21},
title = {{¿Qu{\'{e}} es machine learning? [Gu{\'{i}}a completa para principiantes]}},
url = {https://blog.adext.com/es/machine-learning-guia-completa},
year = {2018}
}
@article{DiDecoSampedro2012,
author = {{Di Deco Sampedro}, Javier},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Di Deco Sampedro - 2012 - E. y A. de t{\'{e}}cnicas de aprendizaje autom{\'{a}}tico en el {\'{a}}mbito m{\'{e}}dico.pdf:pdf},
pages = {103},
title = {{E. y A. de t{\'{e}}cnicas de aprendizaje autom{\'{a}}tico en el {\'{a}}mbito m{\'{e}}dico}},
year = {2012}
}
@misc{HernandezAvila2018,
author = {{Hern{\'{a}}ndez {\'{A}}vila}, Reinier and {P{\'{e}}rez Perdomo}, Eduardo and {S{\'{a}}nchez Vel{\'{a}}zquez}, Luis Miguel and {Gonz{\'{a}}lez Orosco}, Luis David and {Pupo Reyes}, Oscar Gabriel},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez {\'{A}}vila et al. - 2018 - Manual de usuario - clasificacion de imagenes.docx:docx},
pages = {15},
title = {{Manual de usuario - clasificacion de imagenes}},
year = {2018}
}
@book{HernandezAvila2018a,
author = {{Hern{\'{a}}ndez {\'{A}}vila}, Reinier and {P{\'{e}}rez Perdomo}, Eduardo and {S{\'{a}}nchez Vel{\'{a}}zquez}, Luis Miguel and {Gonz{\'{a}}lez Orosco}, Luis David and {Pupo Reyes}, Oscar Gabriel},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez {\'{A}}vila et al. - 2018 - DeepLearning-JCLAL.pdf:pdf},
pages = {14},
title = {{DeepLearning-JCLAL}},
year = {2018}
}
@phdthesis{SanchezVelazquez2017,
abstract = {El Aprendizaje Autom{\'{a}}tico es una rama de la Inteligencia Artificial cuyo objetivo es desarrollar t{\'{e}}cnicas que permitan a las computadoras aprender. A partir de casos pasados se intenta descubrir patrones generalizables para posibles nuevos casos. En este existen varias {\'{a}}reas de estudio, entre ellas se encuentra el Aprendizaje Supervisado, No Supervisado, Semi-Supervisado y Aprendizaje Activo. En la actualidad existen varias herramientas inform{\'{a}}ticas del {\'{a}}rea de Inteligencia Artificial que permiten la aplicaci{\'{o}}n de m{\'{e}}todos y algoritmos de Aprendizaje Autom{\'{a}}tico y sus {\'{a}}reas de estudio para la clasificaci{\'{o}}n de conjuntos de datos, soluci{\'{o}}n de problemas e inferencia de conocimientos. El framework JCLAL es una herramienta inform{\'{a}}tica de c{\'{o}}digo abierto para investigadores y usuarios que apliquen Aprendizaje Activo. JCLAL provee los mecanismos necesarios para desarrollar cualquier t{\'{e}}cnica de Aprendizaje Activo. Aunque JCLAL cuenta con una amplia variedad de elementos positivos como es su portabilidad, elegancia e interoperabilidad, carece de una Interfaz de Usuario al nivel de otros framework que les permitiera a los investigadores realizar los experimentos con solo conocer el flujo general del Aprendizaje Activo y que de esta forma m{\'{a}}s investigadores de la comunidad cient{\'{i}}fica se acerquen al framework. En la presente investigaci{\'{o}}n se propone un entorno de usuario para el framework JCLAL. Se analizan los elementos te{\'{o}}ricos que sustentan el Aprendizaje Activo y el framework JCLAL. Se aplican patrones de dise{\~{n}}o y se dise{\~{n}}a la estructura del entorno de usuario. Por {\'{u}}ltimo, son realizadas pruebas de aceptaci{\'{o}}n para medir la usabilidad de la interfaz propuesta. III},
author = {{S{\'{a}}nchez Vel{\'{a}}zquez}, Luis Miguel},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/S{\'{a}}nchez Vel{\'{a}}zquez - 2017 - VISUALJCLAL ENTORNO DE USUARIO PARA EL FRAMEWORK JCLAL.pdf:pdf},
pages = {66},
school = {Universidad de Holgu{\'{i}}n "Oscar Lucero Moya"},
title = {{VISUALJCLAL : ENTORNO DE USUARIO PARA EL FRAMEWORK JCLAL}},
year = {2017}
}
@phdthesis{PerezPerdomo2013,
abstract = {El aprendizaje autom{\'{a}}tico es una rama de la Inteligencia Artificial, la cual se encarga del estudio de los algoritmos que permiten a las computadoras aprender a partir de experiencia previa. Los algoritmos de clasificaci{\'{o}}n tradicionales emplean solamente ejemplos etiquetados en el proceso de entrenamiento. Sin embargo, en la realidad la obtenci{\'{o}}n de ejemplos etiquetados es una tarea costosa y muchas veces la tarea de etiquetado de datos requiere del esfuerzo de humanos experimentados. El aprendizaje activo es el {\'{a}}rea de estudio que tiene como principal hip{\'{o}}tesis que si el algoritmo de aprendizaje tiene la oportunidad de elegir los datos desde donde aprende, entonces este tendr{\'{a}} una mejor precisi{\'{o}}n con un menor costo de entrenamiento. Un framework en el {\'{a}}rea de las ciencias de la computaci{\'{o}}n es un conjunto de t{\'{e}}cnicas y herramientas que permiten el desarrollo de alg{\'{u}}n producto, abstrayendo y facilitando el desarrollo de ciertas tareas seg{\'{u}}n el dominio para el cual est{\'{a}} construido. Actualmente en el {\'{a}}rea del aprendizaje autom{\'{a}}tico existen frameworks que apoyan el proceso de experimentaci{\'{o}}n y desarrollo de nuevos algoritmos. Sin embargo, los mismos est{\'{a}}n restringidos solamente al {\'{a}}rea del aprendizaje supervisado y no supervisado cl{\'{a}}sico. En la presente investigaci{\'{o}}n se propone un framework para el desarrollo de algoritmos con aprendizaje activo. Se analizan los elementos que integran al aprendizaje activo. Se aplican patrones de dise{\~{n}}o y se dise{\~{n}}a la estructura del framework. Son analizados casos de estudio haciendo uso del framework propuesto y por {\'{u}}ltimo se realiza un estudio de sostenibilidad.},
author = {{P{\'{e}}rez Perdomo}, Eduardo and {Pupo Reyes}, Oscar},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Eduardo/Documento-Eduardo.pdf:pdf},
pages = {113},
school = {Universidad de Holgu{\'{i}}n “Oscar Lucero Moya”},
title = {{PROPUESTA DE UN FRAMEWORK PARA}},
year = {2013}
}
@phdthesis{Hernandez,
author = {Hernandez, Reinier},
file = {:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Hernandez - Unknown - Trabajo de diploma Reinier Hernandez-06-15.docx:docx;:Users/Raymond/Library/Application Support/Mendeley Desktop/Downloaded/Hernandez - Unknown - Trabajo de diploma Reinier Hernandez-06-15(2).docx:docx},
title = {{Trabajo de diploma Reinier Hernandez-06-15}}
}
@phdthesis{PerezPerdomo2016,
abstract = {El presente trabajo se enfoca en el procesamiento y an{\'{a}}lisis de datos, la construcci{\'{o}}n de modelos predictivos y la sugerencia de acciones a ejecutar. Los datos generados en las empresas poseen caracter{\'{i}}sticas que los hacen especialmente utilizables, son datos digitales y estructurados en la mayor{\'{i}}a de los casos. La investigaci{\'{o}}n se centra en el {\'{a}}rea de la exploraci{\'{o}}n minera, en espec{\'{i}}fico en el grupo nacional dedicado al estudio de las potencialidades mineras, conocido como el Instituto de Geolog{\'{i}}a y Paleontolog{\'{i}}a. El instituto se encarga de aprobar los proyectos mineros que se acometen en el pa{\'{i}}s, bas{\'{a}}ndose en el an{\'{a}}lisis de muestras y las caracter{\'{i}}sticas del terreno. Se pretende aplicar m{\'{e}}todos computacionales que analicen los datos hist{\'{o}}ricos y logren incrementar la confianza de las decisiones en la aprobaci{\'{o}}n de proyectos mineros. Las t{\'{e}}cnicas de Aprendizaje Activo permiten seleccionar los casos a partir de donde el modelo aprende, lo cual supone una ventaja en la empresa objeto de estudio ya que la mayor{\'{i}}a de los datos permanecen sin clasificar. No existe un consenso en cu{\'{a}}l combinaci{\'{o}}n de modelo de aprendizaje y estrategia de consulta de Aprendizaje Activo aplicar en el contexto minero. En la presente investigaci{\'{o}}n se desarrolla un amplio estudio experimental que combina seis reconocidos modelos de predicci{\'{o}}n y seis estrategias de consulta, aportando la evidencia necesaria para seleccionar la mejor variante.},
author = {{P{\'{e}}rez Perdomo}, Eduardo},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Eduardo/Eduardo-maipa-2016-lap.pdf:pdf},
pages = {92},
school = {UNIVERSIDAD DE HOLGU{\'{I}}N},
title = {{Construcci{\'{o}}n inteligente de modelos de predicci{\'{o}}n para la mejora del indicador de la predicci{\'{o}}n . Aplicaci{\'{o}}n en proyectos mineros}},
year = {2016}
}
@misc{Lorenita,
author = {Lorenita},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Luis David/Tesis David.docx:docx},
title = {{Tesis David, Lorenita}}
}
@article{Perez2014,
author = {P{\'{e}}rez, Eduardo},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Eduardo/JCLAL- Manual de usuario - imprimir.pdf:pdf},
title = {{JCLAL : FRAMEWORK PARA APRENDIZAJE ACTIVO Manual de Usuario}},
year = {2014}
}
@article{Hoteles2013,
author = {Hoteles, Usuarios Sistema D E},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Eduardo/Manual de usuario.pdf:pdf},
pages = {1--13},
title = {{Manual de usuario}},
year = {2013}
}
@article{PerezPerdomo2015,
abstract = {Machine learning is a branch of Artificial Intelligence, which deals with the study of algorithms that allow computers to learn from previous experience. One area of study of machine learning is active learning, which has as main hypothesis that if the learning algorithm has the opportunity to choose where learning data, then this will have a better accuracy with lower training costs. Currently heart disease are among the leading causes of death, so that early diagnosis of the disease may favor decisions by medical professionals. This work aims to determine how efficient the strategy shows the least reliable in the diagnosis of heart disease.},
author = {{P{\'{e}}rez Perdomo}, Eduardo and {Pupo Reyes}, Oscar Gabriel},
doi = {10.13140/RG.2.1.3229.2009},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Eduardo-Perez-Perdomo{\_}VII CONFERENCIA CIENTIFICA.pdf:pdf},
isbn = {978-959-16-2472-7},
journal = {VII International Scientific Conference of the University of Holguin},
keywords = {aprendizaje,aprendizaje activo,autom{\'{a}}tico,modelo de clasificaci{\'{o}}n},
pages = {1--7},
title = {{Least confident sampling strategy for cardiac diseases diagnosis}},
year = {2015}
}
@misc{,
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Articulo/Articulo CAEPIA.docx:docx},
title = {{Articulo CAEPIA}}
}
@misc{PerezPerdomo,
author = {{P{\'{e}}rez Perdomo}, Eduardo},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/VisualJCLAL-Manual de Usuario.docx:docx},
title = {{VisualJCLAL-Manual de Usuario}}
}
@article{Ja??n2012,
abstract = {This paper presents VisualJCLEC, a visual framework based on JCLEC for Evolutionary Computing. In order to have a high degree of adaptability, the architecture and pattern design followed are focused on enhancing the f exibility and scalability. For illustrative purposes, a case study of an optimization classical problem (the knapsack problem) using this framework is presented, as well as some guidelines on how to add new elements to the environment by means of CDL descriptors. {\textcopyright} 2012 IEEE.},
author = {Ja??n, Juan Ignacio and Romero, Jos?? Ra??l and Ventura, Sebasti??n},
doi = {10.1109/ISDA.2012.6416523},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Articulo/Analysis - 2012 - VisualJCLEC a visual framework for evolutionary computation.pdf:pdf},
isbn = {9781467351188},
issn = {21647143},
journal = {International Conference on Intelligent Systems Design and Applications, ISDA},
keywords = {GUI,JCLEC,evolutionary computation,framework,software tool},
pages = {119--125},
title = {{VisualJCLEC: A visual framework for evolutionary computation}},
year = {2012}
}
@article{Bouckaert2017,
author = {Bouckaert, Remco R and Frank, Eibe and Hall, Mark and Kirkby, Richard and Reutemann, Peter and Seewald, Alex and Scuse, David},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/WekaManual-3-9-2.pdf:pdf},
title = {{WEKA Manual for Version 3-9-2}},
url = {http://www.gnu.org/licenses/gpl-3.0-standalone.html},
year = {2017}
}
@article{Reyes2016,
abstract = {Active Learning has become an important area of research owing to the increasing number of real-world problems which contain labelled and unlabelled examples at the same time. JCLAL is a Java Class Library for Active Learning which has an architecture that follows strong principles of object-oriented design. It is easy to use, and it allows the developers to adapt, modify and extend the framework according to their needs. The library offers a variety of active learning methods that have been proposed in the literature. The software is available under the GPL license.},
author = {Reyes, Oscar and P{\'{e}}rez, Eduardo and Del, Mar{\'{i}}a and Rodr{\'{i}}guez-Hern{\'{a}}ndez, Carmen and Fardoun, Habib M and Ventura, Sebasti{\'{a}}n},
file = {:Users/Raymond/Universidad/TESIS/AL/2016{\_}Reyes et al.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {active learning,framework,java language,object-oriented design},
pages = {1--5},
title = {{JCLAL: A Java Framework for Active Learning}},
url = {http://www.jmlr.org/papers/volume17/15-347/15-347.pdf},
volume = {17},
year = {2016}
}
@article{Rai2011,
abstract = {Piyush},
author = {Rai, Piyush},
doi = {10.1097/01.CCM.0000269938.64692.A0},
file = {:Users/Raymond/Universidad/TESIS/AL/AL.pdf:pdf},
isbn = {9781608457250},
issn = {0090-3493},
journal = {Machine Learning},
keywords = {()},
number = {1},
pmid = {17581367},
title = {{Active Learning Piyush Rai ( Passive ) Supervised Learning}},
volume = {2011},
year = {2011}
}
@article{Eduardo,
author = {Eduardo, P and Gonz, Luis D},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Articulo/JCLALv2{\_}CAEPIA.pdf:pdf},
title = {{herramienta Java de c ´ odigo abierto para el aprendizaje activo}}
}
@misc{SanchezVelazquez2018,
author = {{S{\'{a}}nchez Vel{\'{a}}zquez}, Luis Miguel and {Pupo Reyes}, Oscar Gabriel and {P{\'{e}}rez Perdomo}, Eduardo and {Gonz{\'{a}}lez Orosco}, Luis David},
file = {:Users/Raymond/Universidad/TESIS/JCLAL/Articulo/VisualJCLAL (articulo).docx:docx},
keywords = {GUI,JCLAL,aprendizaje activo,framework},
mendeley-tags = {GUI,JCLAL,aprendizaje activo,framework},
pages = {138},
title = {{VisualJCLAL (articulo)}},
year = {2018}
}
@misc{,
file = {:Users/Raymond/Universidad/TESIS/AL/OReilly - Swing Hacks.chm:chm},
title = {{O'Reilly - Swing Hacks}}
}
@article{Riccardi2005,
abstract = {We are interested in the problem of adaptive learning in the context of automatic speech recognition (ASR). In this paper, we propose an active learning algorithm for ASR. Automatic speech recognition systems are trained using human supervision to provide transcriptions of speech utterances. The goal of Active Learning is to minimize the human supervision for training acoustic and language models and to maximize the performance given the transcribed and untranscribed data. Active learning aims at reducing the number of training examples to be labeled by automatically processing the unlabeled examples, and then selecting the most informative ones with respect to a given cost function for a human to label. In this paper we describe how to estimate the confidence score for each utterance through an on-line algorithm using the lattice output of a speech recognizer. The utterance scores are filtered through the informativeness function and an optimal subset of training samples is selected. The active learning algorithm has been applied to both batch and on-line learning scheme and we have experimented with different selective sampling algorithms. Our experiments show that by using active learning the amount of labeled data needed for a given word accuracy can be reduced by more than 60{\%} with respect to random sampling.},
author = {Riccardi, G},
doi = {10.1109/TSA.2005.848882},
file = {:Users/Raymond/Universidad/TESIS/AL/tong{\_}thesis.pdf:pdf},
issn = {10636676},
journal = {Ieee Transactions On Speech And Audio Processing},
number = {4},
pages = {2105--511},
title = {{Active learning: theory and applications to automatic speech recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1453593},
volume = {15},
year = {2005}
}
@book{Mitchell1997,
abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Mitchell, Tom},
booktitle = {Machine Learning},
doi = {10.1007/BF00116892},
eprint = {0-387-31073-8},
file = {:Users/Raymond/Universidad/TESIS/AL/Machine Learning.pdf:pdf},
isbn = {0070428077},
issn = {08856125},
pages = {432},
pmid = {20236947},
title = {{Machine Learning}},
year = {1997}
}
@article{Garcia2008,
abstract = {Este ave da nombre a una extensa colecci{\'{o}}n de algoritmos de M{\'{a}}quinas de conocimiento desarrollados por la universidad de Waikato (Nueva Zelanda) implementados en Java [1, 2]; {\'{u}}tiles para ser aplicados sobre datos mediante los interfaces que ofrece o para embeberlos dentro de cualquier aplicaci{\'{o}}n. Adem{\'{a}}s Weka contiene las herramientas necesarias para realizar transformaciones sobre los datos, tareas de clasiﬁcaci{\'{o}}n, regresi{\'{o}}n, clustering, asociaci{\'{o}}n y visualizaci{\'{o}}n. Weka est{\'{a}} dise{\~{n}}ado como una herramienta orientada a la extensibilidad por lo que a{\~{n}}adir nuevas funcionalidades es una tarea sencilla.},
author = {Garc{\'{i}}a, Diego},
file = {:Users/Raymond/Universidad/TESIS/AL/Manual de weka.pdf:pdf},
isbn = {9788591407606},
journal = {Dispon{\'{i}}vel atrav{\'{e}}s do e-mail diego. garcia. {\ldots}},
title = {{Manual de WEKA}},
year = {2008}
}
@article{Settles2010b,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
eprint = {1206.5533},
file = {:Users/Raymond/Universidad/TESIS/AL/Unknown{\_}SETTLES{\_}Active Learning Literature Survey{\_}2010.pdf:pdf},
isbn = {1648},
issn = {00483931},
journal = {University of Wisconsin, Madison},
number = {2},
pages = {201--221},
pmid = {15003161},
title = {{Active learning literature survey}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.4245{\&}rep=rep1{\&}type=pdf},
volume = {15},
year = {2010}
}
